# K-Nearest Neighbors (KNN) from Scratch â€” Python & NumPy

This repository contains a **from-scratch implementation of the K-Nearest Neighbors (KNN) algorithm**  
using only **Python and NumPy**, without any machine learning libraries.

It includes a **realistic synthetic dataset of houses** with multiple features, scaling, and optional PCA visualization for insights.

---

## ğŸ¯ Goal

To deeply understand:

- How distance-based classification works
- The role of similarity metrics in supervised learning
- How the choice of **K** affects biasâ€“variance tradeoff
- Why KNN has no explicit training phase
- How preprocessing (scaling, PCA) affects model performance

## ğŸ§  Algorithm Overview

KNN is a **supervised, non-parametric, lazy learning** algorithm.

### Core Idea

1. Store all training samples
2. For a new input point:
   - Compute distance to all training points (**Euclidean** or **Manhattan**)
   - Select the **K nearest neighbors**
   - Predict the label using **majority voting**

## âœï¸ Mathematical Intuition

### Distance Metrics

- **Euclidean distance**:

> d(x, xáµ¢) = âˆš(âˆ‘(x âˆ’ xáµ¢)Â²)

- **Manhattan distance**:

> d(x,y)=âˆ‘âˆ£xiâ€‹âˆ’yiâ€‹âˆ£

### Prediction Rule
Å· = most frequent label among the K nearest neighbors

---

## ğŸ¡ Dataset Scenario

- Synthetic housing dataset with **1200 samples**
- Features:
  - Area (mÂ²), Rooms, Building Age, Distance to City Center
  - Floor, Elevator, Parking
- Target: **Price Category (5 classes: Economy â†’ Luxury)**
- Includes realistic noise and overlapping classes
- Suitable for KNN, visualization, and PCA exploration

## ğŸ“Š Observations & Results

- **Small K values**:
  - Low bias
  - High variance
  - Sensitive to noise
- **Large K values**:
  - Smoother decision boundary
  - Potential underfitting
- Scaling and PCA clearly show **clusters and separability**
- Distance metric choice affects nearest neighbor selection

## âœ… Advantages

- Very simple to understand and implement
- No explicit training phase
- Works well on small to medium datasets
- Highly interpretable
- Provides clear insight with visualization

## âŒ Disadvantages

- Slow inference on large datasets (**O(n)**)
- High memory usage (stores all data)
- Sensitive to noisy data
- Strongly affected by feature scaling
- Choosing K is non-trivial

## ğŸ›  Tech Stack

- Python  
- NumPy (for calculation and data manipulation)  
- Matplotlib (for visualization and debugging)  
- Scikit-learn **only for StandardScaler / PCA** (optional)

---

## ğŸš€ Why this project?

This project demonstrates **understanding, not abstraction**.  
It shows my ability to:

- Implement algorithms from scratch
- Handle preprocessing and dimensionality reduction
- Reason about biasâ€“variance tradeoff and distance metrics
- Visualize high-dimensional data intuitively
